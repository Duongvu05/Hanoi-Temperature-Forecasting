{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a308cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FEATURE ENGINEERING SETUP\n",
      "===================================\n",
      "‚úÖ Libraries imported successfully!\n",
      "üìä Pandas version: 2.2.3\n",
      "ü§ñ Scikit-learn available for feature selection\n"
     ]
    }
   ],
   "source": [
    "#Import libraries for feature engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest\n",
    "import warnings\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"üîß FEATURE ENGINEERING SETUP\")\n",
    "print(\"=\" * 35)\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"ü§ñ Scikit-learn available for feature selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42abe518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load cleaned data (from data processing step)\n",
    "df = pd.read_csv('../data/processed/prepocessed_daily_data.csv')  # Will be updated to use processed data\n",
    "df.set_index('datetime', inplace=True)\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04245510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sunrise'] = pd.to_datetime(df['sunrise'])\n",
    "df['sunset'] = pd.to_datetime(df['sunset'])\n",
    "df['day_length_hours'] = df['sunset'] - df['sunrise']\n",
    "df = df.drop(columns=['sunrise', 'sunset'])\n",
    "df['day_length_hours'] = df['day_length_hours'].dt.total_seconds() / 3600.0\n",
    "\n",
    "df['target'] = df['temp'].shift(-5)\n",
    "df = df[~df['target'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b61955ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_solar_interaction\"] = df[\"temp\"] * df[\"solarradiation\"]\n",
    "df[\"uv_temp_interaction\"] = df[\"uvindex\"] * df[\"temp\"]\n",
    "df['temp_cloudcover_interaction'] = df['temp'] * df['cloudcover']\n",
    "df['temp_sealevelpressure_interaction'] = df['temp'] * df['sealevelpressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1be2959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of NaN values in each column after handling rolling horizons and lagging:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "The dataframe does not contain any NaN values.\n"
     ]
    }
   ],
   "source": [
    "# Create lagging features\n",
    "def create_lag_features(df, cols, lags):\n",
    "    for col in cols:\n",
    "        for lag in lags:\n",
    "            df[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Specify columns and lags\n",
    "# Get all numerical columns\n",
    "computing_columns = df.drop(columns=['year', 'month', 'day', 'day_of_year', 'season', 'is_rainy', 'target']).columns\n",
    "\n",
    "lag_steps = [1, 2, 3, 5, 7, 10, 14]  # Example lag steps\n",
    "\n",
    "# Apply lagging features before handling rolling horizons\n",
    "df = create_lag_features(df, computing_columns, lag_steps)\n",
    "\n",
    "# Function to compute rolling mean and percentage change\n",
    "def compute_rolling(df, horizon, col):\n",
    "    label = f\"rolling_{horizon}_{col}\"\n",
    "    df[label] = df[col].rolling(horizon, min_periods=horizon).mean()  # Ensure full horizon is used\n",
    "    df[f\"{label}_change\"] = df[col] - df[label]\n",
    "    return df\n",
    "\n",
    "# Compute rolling features for specified horizons\n",
    "rolling_horizons = [3, 7, 14]  # Rolling windows of 3, 7, 14 days\n",
    "for horizon in rolling_horizons:\n",
    "    for col in computing_columns:\n",
    "        df = compute_rolling(df, horizon, col)\n",
    "\n",
    "# Drop rows with NaN values caused by rolling horizons\n",
    "df = df.iloc[14:]\n",
    "# Verify no NaN values exist\n",
    "nan_summary = df.isna().sum()\n",
    "print(\"Summary of NaN values in each column after handling rolling horizons and lagging:\")\n",
    "print(nan_summary[nan_summary > 0])\n",
    "\n",
    "if df.isna().any().any():\n",
    "    print(\"\\nThe dataframe contains NaN values.\")\n",
    "else:\n",
    "    print(\"\\nThe dataframe does not contain any NaN values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be9aa3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Months and days average\n",
    "def expand_mean(df):\n",
    "    return df.expanding(1).mean()\n",
    "\n",
    "for col in computing_columns:\n",
    "    df[f\"month_avg_{col}\"] = df[col].groupby(df.index.month, group_keys=False).apply(expand_mean)\n",
    "    df[f\"day_avg_{col}\"] = df[col].groupby(df.index.day_of_year, group_keys=False).apply(expand_mean)\n",
    "    df[f\"year_avg_{col}\"] = df[col].groupby(df.index.year, group_keys=False).apply(expand_mean)\n",
    "    df[f\"season_avg_{col}\"] = df[col].groupby(df['season'], group_keys=False).apply(expand_mean)\n",
    "    df[\"month_max_temp\"] = df['temp'].groupby(df.index.month, group_keys=False).cummax()\n",
    "    df[\"month_min_temp\"] = df['temp'].groupby(df.index.month, group_keys=False).cummin()\n",
    "\n",
    "df[\"temp_volatility_7\"] = df[\"temp\"].rolling(7).std()\n",
    "df[\"temp_volatility_30\"] = df[\"temp\"].rolling(30).std()\n",
    "df[\"temp_spike_flag\"] = (df[\"temp\"] - df[\"temp\"].shift(1)).abs() > 5\n",
    "df[\"temp_anomaly_vs_month_avg\"] = df[\"temp\"] - df[\"month_avg_temp\"]\n",
    "df[\"temp_anomaly_vs_season_avg\"] = df[\"temp\"] - df[\"season_avg_temp\"]\n",
    "\n",
    "df[\"pressure_trend_3d\"] = df[\"sealevelpressure\"] - df[\"sealevelpressure\"].shift(3)\n",
    "df[\"pressure_trend_7d\"] = df[\"sealevelpressure\"] - df[\"sealevelpressure\"].shift(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef6fec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3635, 502)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "460ecb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/feature_engineering_daily_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30dccab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library for model training\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "import optuna\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Loading processed data\n",
    "df = pd.read_csv('../data/processed/feature_engineering_daily_data2.csv', index_col='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ec346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# ---------- 1) Kh√≥a schema OHE to√†n c·ª•c ----------\n",
    "ohe_template = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "ohe_template.fit(X[cat_cols])\n",
    "fixed_categories = ohe_template.categories_\n",
    "\n",
    "def make_preprocessor():\n",
    "    num_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore',\n",
    "                              sparse_output=False,\n",
    "                              categories=fixed_categories))\n",
    "    ])\n",
    "    return ColumnTransformer([\n",
    "        ('num', num_pipe, num_cols),\n",
    "        ('cat', cat_pipe, cat_cols)\n",
    "    ], remainder='drop')\n",
    "\n",
    "# ---------- 2) C·ªë ƒë·ªãnh m·∫∑t n·∫° VarianceThreshold t·ª´ FOLD ƒê·∫¶U ----------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "first_train_idx, first_test_idx = next(tscv.split(X))\n",
    "\n",
    "# Fit preprocessor + VT tr√™n fold ƒë·∫ßu\n",
    "preprocessor = make_preprocessor()\n",
    "preprocessor.fit(X.iloc[first_train_idx], y.iloc[first_train_idx])\n",
    "X_first_train_trans = preprocessor.transform(X.iloc[first_train_idx])\n",
    "vt = VarianceThreshold(threshold=0.0).fit(X_first_train_trans)\n",
    "vt_support_mask = vt.get_support()\n",
    "feat_names_all = preprocessor.get_feature_names_out()\n",
    "feat_names_after_vt = feat_names_all[vt_support_mask]\n",
    "\n",
    "def apply_vt_mask(X_mat, mask=vt_support_mask):\n",
    "    return X_mat[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "75112486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 00:20:13,373] A new study created in memory with name: no-name-2afe0506-b4aa-462b-8f01-80387ed96659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫ÆT ƒê·∫¶U TUNING XGBoost v·ªõi Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 00:20:45,095] Trial 0 finished with value: 0.6841546321170424 and parameters: {'n_estimators': 177, 'max_depth': 12, 'learning_rate': 0.08952457031201479, 'subsample': 0.8125433978634484, 'colsample_bytree': 0.7864971793172957, 'min_child_weight': 8, 'reg_alpha': 8.938554972762976, 'reg_lambda': 8.94144419417929}. Best is trial 0 with value: 0.6841546321170424.\n",
      "[I 2025-10-27 00:21:27,783] Trial 1 finished with value: 0.6855786113860656 and parameters: {'n_estimators': 375, 'max_depth': 7, 'learning_rate': 0.10284749546001738, 'subsample': 0.5531401147361343, 'colsample_bytree': 0.8600006752638711, 'min_child_weight': 10, 'reg_alpha': 6.619475160118753, 'reg_lambda': 5.1420312695275685}. Best is trial 1 with value: 0.6855786113860656.\n",
      "[I 2025-10-27 00:24:51,276] Trial 2 finished with value: 0.6832774509727118 and parameters: {'n_estimators': 816, 'max_depth': 12, 'learning_rate': 0.06564928958505832, 'subsample': 0.5061200810361614, 'colsample_bytree': 0.7867643784889164, 'min_child_weight': 1, 'reg_alpha': 0.4251954864274321, 'reg_lambda': 5.2820821793686275}. Best is trial 1 with value: 0.6855786113860656.\n",
      "[I 2025-10-27 00:25:57,052] Trial 3 finished with value: 0.6892987319121737 and parameters: {'n_estimators': 491, 'max_depth': 8, 'learning_rate': 0.029310259199634484, 'subsample': 0.9799857545810473, 'colsample_bytree': 0.7552324490364112, 'min_child_weight': 9, 'reg_alpha': 5.608370196209107, 'reg_lambda': 1.9484886385860756}. Best is trial 3 with value: 0.6892987319121737.\n",
      "[I 2025-10-27 00:26:11,198] Trial 4 finished with value: 0.7214184486962771 and parameters: {'n_estimators': 550, 'max_depth': 4, 'learning_rate': 0.022183906929384153, 'subsample': 0.6248307693442694, 'colsample_bytree': 0.9636208095326908, 'min_child_weight': 3, 'reg_alpha': 5.499914111414458, 'reg_lambda': 5.403757368224156}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:27:48,900] Trial 5 finished with value: 0.6908673098995698 and parameters: {'n_estimators': 524, 'max_depth': 12, 'learning_rate': 0.027860176255891548, 'subsample': 0.5785673732404576, 'colsample_bytree': 0.5923692922584057, 'min_child_weight': 3, 'reg_alpha': 4.479840724712133, 'reg_lambda': 9.673903516445492}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:28:26,672] Trial 6 finished with value: 0.645861681776928 and parameters: {'n_estimators': 693, 'max_depth': 9, 'learning_rate': 0.2630383580669409, 'subsample': 0.5506709514413439, 'colsample_bytree': 0.561273872301402, 'min_child_weight': 6, 'reg_alpha': 8.121622515265779, 'reg_lambda': 8.055838928466049}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:29:58,630] Trial 7 finished with value: 0.6778186472293866 and parameters: {'n_estimators': 261, 'max_depth': 9, 'learning_rate': 0.011163596696035937, 'subsample': 0.9302159341924033, 'colsample_bytree': 0.8102198645362417, 'min_child_weight': 4, 'reg_alpha': 2.4003548066930547, 'reg_lambda': 7.434013397579378}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:30:28,980] Trial 8 finished with value: 0.6753700562095334 and parameters: {'n_estimators': 953, 'max_depth': 7, 'learning_rate': 0.12180519717190377, 'subsample': 0.8919860865918814, 'colsample_bytree': 0.7901969001020941, 'min_child_weight': 5, 'reg_alpha': 3.389253525370023, 'reg_lambda': 5.512553545070878}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:30:48,334] Trial 9 finished with value: 0.6562011088173904 and parameters: {'n_estimators': 304, 'max_depth': 12, 'learning_rate': 0.17142413744435286, 'subsample': 0.9395698188154478, 'colsample_bytree': 0.7160350007097818, 'min_child_weight': 4, 'reg_alpha': 9.938223004588579, 'reg_lambda': 3.4361700518219473}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:30:49,946] Trial 10 finished with value: 0.553481795073276 and parameters: {'n_estimators': 74, 'max_depth': 3, 'learning_rate': 0.011107340785274585, 'subsample': 0.6818832136218628, 'colsample_bytree': 0.989906791325797, 'min_child_weight': 2, 'reg_alpha': 6.909029343710506, 'reg_lambda': 0.5956521319837869}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:30:57,702] Trial 11 finished with value: 0.7192897858952213 and parameters: {'n_estimators': 613, 'max_depth': 3, 'learning_rate': 0.02863630318401424, 'subsample': 0.6589942180528668, 'colsample_bytree': 0.5002919454235272, 'min_child_weight': 3, 'reg_alpha': 4.24952253478539, 'reg_lambda': 9.78019829655598}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:31:08,064] Trial 12 finished with value: 0.7184555141151278 and parameters: {'n_estimators': 667, 'max_depth': 3, 'learning_rate': 0.027667873346632788, 'subsample': 0.6792847679319075, 'colsample_bytree': 0.99172204162636, 'min_child_weight': 1, 'reg_alpha': 4.2472477696642645, 'reg_lambda': 7.282600161584803}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:31:32,687] Trial 13 finished with value: 0.7098159287175231 and parameters: {'n_estimators': 676, 'max_depth': 5, 'learning_rate': 0.04127312909481688, 'subsample': 0.6619836095123793, 'colsample_bytree': 0.6519865783051391, 'min_child_weight': 6, 'reg_alpha': 2.284354254047346, 'reg_lambda': 3.18583042408264}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:31:49,843] Trial 14 finished with value: 0.7119798665740666 and parameters: {'n_estimators': 409, 'max_depth': 5, 'learning_rate': 0.01725897786749662, 'subsample': 0.7595432762025505, 'colsample_bytree': 0.8947466624808069, 'min_child_weight': 3, 'reg_alpha': 5.689852106838459, 'reg_lambda': 6.324083915624578}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:32:11,934] Trial 15 finished with value: 0.7122508739877109 and parameters: {'n_estimators': 601, 'max_depth': 5, 'learning_rate': 0.021488497736425718, 'subsample': 0.6327689273747077, 'colsample_bytree': 0.5128362929719139, 'min_child_weight': 3, 'reg_alpha': 2.9497608055986007, 'reg_lambda': 9.960617170333874}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:32:31,964] Trial 16 finished with value: 0.7090317885571829 and parameters: {'n_estimators': 918, 'max_depth': 4, 'learning_rate': 0.052096367728097365, 'subsample': 0.7457941884934057, 'colsample_bytree': 0.6796185215814209, 'min_child_weight': 7, 'reg_alpha': 1.2660259666845288, 'reg_lambda': 4.128166897513019}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:32:52,042] Trial 17 finished with value: 0.7174633913522255 and parameters: {'n_estimators': 817, 'max_depth': 4, 'learning_rate': 0.016345549418247476, 'subsample': 0.6132249990567036, 'colsample_bytree': 0.8808520839528741, 'min_child_weight': 5, 'reg_alpha': 7.131330348790661, 'reg_lambda': 6.507436691558155}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:33:18,367] Trial 18 finished with value: 0.7060487908226645 and parameters: {'n_estimators': 469, 'max_depth': 6, 'learning_rate': 0.0414441627894837, 'subsample': 0.7311507776125307, 'colsample_bytree': 0.6428623318184143, 'min_child_weight': 2, 'reg_alpha': 5.18108482536489, 'reg_lambda': 0.6314569703335247}. Best is trial 4 with value: 0.7214184486962771.\n",
      "[I 2025-10-27 00:33:25,375] Trial 19 finished with value: 0.7221694062267612 and parameters: {'n_estimators': 590, 'max_depth': 3, 'learning_rate': 0.016243423070598156, 'subsample': 0.8465769758662015, 'colsample_bytree': 0.5047693100618008, 'min_child_weight': 4, 'reg_alpha': 3.809673092180806, 'reg_lambda': 8.416944018324408}. Best is trial 19 with value: 0.7221694062267612.\n",
      "[I 2025-10-27 00:33:56,986] Trial 20 finished with value: 0.7138738191035241 and parameters: {'n_estimators': 775, 'max_depth': 4, 'learning_rate': 0.015780883914464628, 'subsample': 0.8359889101734786, 'colsample_bytree': 0.9407051167816056, 'min_child_weight': 4, 'reg_alpha': 1.4273612135257485, 'reg_lambda': 8.153197851468246}. Best is trial 19 with value: 0.7221694062267612.\n",
      "[I 2025-10-27 00:34:16,444] Trial 21 finished with value: 0.7186469778438231 and parameters: {'n_estimators': 589, 'max_depth': 3, 'learning_rate': 0.021921679495710156, 'subsample': 0.8252806955450737, 'colsample_bytree': 0.5475484261555105, 'min_child_weight': 2, 'reg_alpha': 3.7876878204714988, 'reg_lambda': 8.615527801599098}. Best is trial 19 with value: 0.7221694062267612.\n",
      "[I 2025-10-27 00:34:35,831] Trial 22 finished with value: 0.7162967231415683 and parameters: {'n_estimators': 596, 'max_depth': 3, 'learning_rate': 0.03653137065859918, 'subsample': 0.7208259213485985, 'colsample_bytree': 0.5018572936640041, 'min_child_weight': 4, 'reg_alpha': 4.770035207837714, 'reg_lambda': 9.37452713312871}. Best is trial 19 with value: 0.7221694062267612.\n",
      "[I 2025-10-27 00:34:58,116] Trial 23 finished with value: 0.7200780422299785 and parameters: {'n_estimators': 424, 'max_depth': 4, 'learning_rate': 0.014167506725220413, 'subsample': 0.7996327603682419, 'colsample_bytree': 0.6111022141411077, 'min_child_weight': 3, 'reg_alpha': 6.3250240780129126, 'reg_lambda': 6.561817226091168}. Best is trial 19 with value: 0.7221694062267612.\n",
      "[W 2025-10-27 00:35:14,301] Trial 24 failed with parameters: {'n_estimators': 442, 'max_depth': 6, 'learning_rate': 0.010076763000000587, 'subsample': 0.8773326350769808, 'colsample_bytree': 0.6016178277804242, 'min_child_weight': 5, 'reg_alpha': 6.007819110536611, 'reg_lambda': 6.615914928406993} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_11720\\2978542511.py\", line 39, in objective\n",
      "    model.fit(X_train_t, y_train)\n",
      "  File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 2247, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-27 00:35:14,302] Trial 24 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[489], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# T·∫°o study v√† t·ªëi ∆∞u h√≥a\u001b[39;00m\n\u001b[0;32m     47\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# T∆∞∆°ng ƒë∆∞∆°ng n_iter=100\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# L·∫•y tham s·ªë t·ªët nh·∫•t\u001b[39;00m\n\u001b[0;32m     51\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    257\u001b[0m ):\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[489], line 39\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     36\u001b[0m X_test_t \u001b[38;5;241m=\u001b[39m apply_vt_mask(prep_tune\u001b[38;5;241m.\u001b[39mtransform(X_test))\n\u001b[0;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_t)\n\u001b[0;32m     41\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2246\u001b[0m     _check_call(\n\u001b[1;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2250\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"B·∫ÆT ƒê·∫¶U TUNING XGBoost v·ªõi Optuna...\")\n",
    "\n",
    "# D√πng 70-80% d·ªØ li·ªáu ƒë·∫ßu ƒë·ªÉ tuning\n",
    "train_ratio = 0.8\n",
    "split_idx = int(train_ratio * len(X))\n",
    "X_tune, y_tune = X.iloc[:split_idx], y.iloc[:split_idx]\n",
    "\n",
    "# Preprocess c·ªë ƒë·ªãnh\n",
    "prep_tune = make_preprocessor()\n",
    "prep_tune.fit(X_tune, y_tune)\n",
    "X_tune_t = apply_vt_mask(prep_tune.transform(X_tune))\n",
    "\n",
    "# H√†m objective cho Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    all_r2 = []\n",
    "    for tr_idx, te_idx in tscv.split(X_tune):\n",
    "        X_train, X_test = X_tune.iloc[tr_idx], X_tune.iloc[te_idx]\n",
    "        y_train, y_test = y_tune.iloc[tr_idx], y_tune.iloc[te_idx]\n",
    "\n",
    "        X_train_t = apply_vt_mask(prep_tune.transform(X_train))\n",
    "        X_test_t = apply_vt_mask(prep_tune.transform(X_test))\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train_t, y_train)\n",
    "        y_pred = model.predict(X_test_t)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        all_r2.append(r2)\n",
    "\n",
    "    return np.mean(all_r2)\n",
    "\n",
    "# T·∫°o study v√† t·ªëi ∆∞u h√≥a\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=35)  # T∆∞∆°ng ƒë∆∞∆°ng n_iter=100\n",
    "\n",
    "# L·∫•y tham s·ªë t·ªët nh·∫•t\n",
    "best_params = study.best_params\n",
    "print(\"HO√ÄN TH√ÄNH TUNING! R2 t·ªët nh·∫•t:\", study.best_value)\n",
    "print(\"Tham s·ªë t·ªët nh·∫•t:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e62247a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 487, 'max_depth': 7, 'learning_rate': 0.011293849270387353, 'subsample': 0.6871705314157253, 'colsample_bytree': 0.5317995526559421, 'min_child_weight': 8, 'reg_alpha': 0.6342918304930286, 'reg_lambda': 1.235926349200246}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "959550a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV V√íNG 1: D√ôNG MODEL ƒê√É TUNED + Permutation Importance ===\n",
      "Fold 1: RMSE=2.831  R2=0.692\n",
      "Fold 2: RMSE=2.601  R2=0.703\n",
      "Fold 3: RMSE=2.586  R2=0.757\n",
      "Fold 4: RMSE=2.621  R2=0.736\n",
      "Fold 5: RMSE=2.337  R2=0.771\n",
      "\n",
      "=== K·∫æT QU·∫¢ T·ªîNG K·∫æT CV V√íNG 1 ===\n",
      "Avg RMSE: 2.595 ¬± 0.157\n",
      "Avg MAE: 2.050 ¬± 0.114\n",
      "Avg R2: 0.732 ¬± 0.030\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3) CV v√≤ng 1: D√ôNG MODEL ƒê√É TUNED (KH√îNG TUNING L·∫†I) ----------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "all_rmse, all_mae, all_r2 = [], [], []\n",
    "perm_importances_list = []\n",
    "\n",
    "print(\"\\n=== CV V√íNG 1: D√ôNG MODEL ƒê√É TUNED + Permutation Importance ===\")\n",
    "for fold, (tr_idx, te_idx) in enumerate(tscv.split(X), start=1):\n",
    "    X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    y_train, y_test = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    preprocessor = make_preprocessor()\n",
    "    preprocessor.fit(X_train, y_train)\n",
    "    X_train_t = apply_vt_mask(preprocessor.transform(X_train))\n",
    "    X_test_t  = apply_vt_mask(preprocessor.transform(X_test))\n",
    "\n",
    "    # D√ôNG MODEL ƒê√É TUNED\n",
    "    model = XGBRegressor(**best_params, random_state=42, n_jobs=-1, tree_method='hist')\n",
    "    model.fit(X_train_t, y_train)\n",
    "    y_pred = model.predict(X_test_t)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    all_rmse.append(rmse); all_mae.append(mae); all_r2.append(r2)\n",
    "\n",
    "    # Permutation Importance\n",
    "    perm = permutation_importance(model, X_test_t, y_test, n_repeats=3, random_state=42, n_jobs=-1)\n",
    "    perm_importances_list.append(perm.importances_mean)\n",
    "\n",
    "    print(f\"Fold {fold}: RMSE={rmse:.3f}  R2={r2:.3f}\")\n",
    "\n",
    "print(\"\\n=== K·∫æT QU·∫¢ T·ªîNG K·∫æT CV V√íNG 1 ===\")\n",
    "print(f\"Avg RMSE: {np.mean(all_rmse):.3f} ¬± {np.std(all_rmse):.3f}\")\n",
    "print(f\"Avg MAE: {np.mean(all_mae):.3f} ¬± {np.std(all_mae):.3f}\")\n",
    "print(f\"Avg R2: {np.mean(all_r2):.3f} ¬± {np.std(all_r2):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15ea4ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-100 features ƒë∆∞·ª£c ch·ªçn t·ª´ model ƒë√£ tuning.\n"
     ]
    }
   ],
   "source": [
    "# === CH·ªåN TOP-K T·ª™ TRUNG B√åNH ===\n",
    "avg_perm = np.mean(perm_importances_list, axis=0)\n",
    "order = np.argsort(avg_perm)[::-1]\n",
    "cumsum = np.cumsum(avg_perm[order]); cumsum /= cumsum[-1]\n",
    "top_k = 100\n",
    "top_idx = order[:top_k]\n",
    "\n",
    "print(f\"\\nTop-{top_k} features ƒë∆∞·ª£c ch·ªçn t·ª´ model ƒë√£ tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c83c89f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 09:15:25,496] A new study created in memory with name: no-name-d2b3897d-2986-4061-be1d-ee4984988aa9\n",
      "[I 2025-10-27 09:15:40,081] Trial 0 finished with value: 0.719422500697122 and parameters: {'n_estimators': 933, 'max_depth': 8, 'learning_rate': 0.02398396562062577, 'subsample': 0.6323781317397077, 'colsample_bytree': 0.9999418263101227, 'min_child_weight': 9, 'reg_alpha': 9.931189638306169, 'reg_lambda': 0.9417948215149696}. Best is trial 0 with value: 0.719422500697122.\n",
      "[I 2025-10-27 09:15:45,230] Trial 1 finished with value: 0.6898208443397305 and parameters: {'n_estimators': 642, 'max_depth': 8, 'learning_rate': 0.2363802949508219, 'subsample': 0.8643734905056762, 'colsample_bytree': 0.705557947955507, 'min_child_weight': 4, 'reg_alpha': 7.605368050772253, 'reg_lambda': 7.565812542942202}. Best is trial 0 with value: 0.719422500697122.\n",
      "[I 2025-10-27 09:15:47,849] Trial 2 finished with value: 0.7244889689322986 and parameters: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.027083450603802645, 'subsample': 0.9091594276024612, 'colsample_bytree': 0.8174987361823685, 'min_child_weight': 7, 'reg_alpha': 9.688646338697003, 'reg_lambda': 0.6176752331596935}. Best is trial 2 with value: 0.7244889689322986.\n",
      "[I 2025-10-27 09:15:54,780] Trial 3 finished with value: 0.6826993123033215 and parameters: {'n_estimators': 260, 'max_depth': 10, 'learning_rate': 0.12400423691815841, 'subsample': 0.633225173237608, 'colsample_bytree': 0.8219956894103064, 'min_child_weight': 1, 'reg_alpha': 9.132261414263546, 'reg_lambda': 7.450993292018088}. Best is trial 2 with value: 0.7244889689322986.\n",
      "[I 2025-10-27 09:15:56,313] Trial 4 finished with value: 0.6937837368025873 and parameters: {'n_estimators': 160, 'max_depth': 4, 'learning_rate': 0.2661495139937753, 'subsample': 0.7510942029598258, 'colsample_bytree': 0.9221483640287083, 'min_child_weight': 5, 'reg_alpha': 8.177083382376818, 'reg_lambda': 6.035392976100137}. Best is trial 2 with value: 0.7244889689322986.\n",
      "[I 2025-10-27 09:15:58,847] Trial 5 finished with value: 0.6892307781391616 and parameters: {'n_estimators': 160, 'max_depth': 6, 'learning_rate': 0.25870179216291705, 'subsample': 0.5454568598911762, 'colsample_bytree': 0.6602789903794744, 'min_child_weight': 1, 'reg_alpha': 0.5550836761404077, 'reg_lambda': 5.322151386181183}. Best is trial 2 with value: 0.7244889689322986.\n",
      "[I 2025-10-27 09:16:08,630] Trial 6 finished with value: 0.7137976159587565 and parameters: {'n_estimators': 552, 'max_depth': 8, 'learning_rate': 0.09072031362842709, 'subsample': 0.6286695184564595, 'colsample_bytree': 0.5540721559616919, 'min_child_weight': 7, 'reg_alpha': 6.926693177533066, 'reg_lambda': 5.87242550866169}. Best is trial 2 with value: 0.7244889689322986.\n",
      "[I 2025-10-27 09:16:27,180] Trial 7 finished with value: 0.7192926701571787 and parameters: {'n_estimators': 731, 'max_depth': 8, 'learning_rate': 0.02889927836028732, 'subsample': 0.9466156315023347, 'colsample_bytree': 0.7665383338105435, 'min_child_weight': 8, 'reg_alpha': 4.731200059846654, 'reg_lambda': 1.6823042126087362}. Best is trial 2 with value: 0.7244889689322986.\n",
      "[I 2025-10-27 09:16:29,280] Trial 8 finished with value: 0.7331150233416908 and parameters: {'n_estimators': 234, 'max_depth': 5, 'learning_rate': 0.05053260378713654, 'subsample': 0.6470407053549956, 'colsample_bytree': 0.5653949276735131, 'min_child_weight': 10, 'reg_alpha': 7.101211176506731, 'reg_lambda': 2.529748165668977}. Best is trial 8 with value: 0.7331150233416908.\n",
      "[I 2025-10-27 09:16:34,509] Trial 9 finished with value: 0.7226020738941529 and parameters: {'n_estimators': 793, 'max_depth': 4, 'learning_rate': 0.059952378902325326, 'subsample': 0.5658595897880745, 'colsample_bytree': 0.6533622811992461, 'min_child_weight': 9, 'reg_alpha': 8.78128461431458, 'reg_lambda': 2.072139005010852}. Best is trial 8 with value: 0.7331150233416908.\n",
      "[I 2025-10-27 09:16:38,930] Trial 10 finished with value: 0.7444379945905113 and parameters: {'n_estimators': 384, 'max_depth': 3, 'learning_rate': 0.012318039653113485, 'subsample': 0.7768406490663968, 'colsample_bytree': 0.5083637777871188, 'min_child_weight': 10, 'reg_alpha': 4.260197353407112, 'reg_lambda': 3.280289904635101}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:16:43,266] Trial 11 finished with value: 0.742299270856224 and parameters: {'n_estimators': 382, 'max_depth': 3, 'learning_rate': 0.0109981868881535, 'subsample': 0.7600273365792267, 'colsample_bytree': 0.5036799685124193, 'min_child_weight': 10, 'reg_alpha': 4.563095589121774, 'reg_lambda': 3.7126409540856162}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:16:47,830] Trial 12 finished with value: 0.7421706852619554 and parameters: {'n_estimators': 407, 'max_depth': 3, 'learning_rate': 0.010148552744900407, 'subsample': 0.7890861706961336, 'colsample_bytree': 0.5203603378700613, 'min_child_weight': 10, 'reg_alpha': 4.076307982951437, 'reg_lambda': 3.6799857624865306}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:16:52,680] Trial 13 finished with value: 0.7412574937314275 and parameters: {'n_estimators': 416, 'max_depth': 3, 'learning_rate': 0.010213593471691117, 'subsample': 0.8287608236667636, 'colsample_bytree': 0.60199048265066, 'min_child_weight': 3, 'reg_alpha': 2.980233807626265, 'reg_lambda': 3.893785536908882}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:16:55,113] Trial 14 finished with value: 0.7427082558631135 and parameters: {'n_estimators': 417, 'max_depth': 4, 'learning_rate': 0.01587221308931004, 'subsample': 0.7421418039594114, 'colsample_bytree': 0.5102948323921463, 'min_child_weight': 7, 'reg_alpha': 2.0776093265089517, 'reg_lambda': 3.633369939050194}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:16:56,325] Trial 15 finished with value: 0.5801005200974855 and parameters: {'n_estimators': 55, 'max_depth': 5, 'learning_rate': 0.01690369898515162, 'subsample': 0.6987252995482324, 'colsample_bytree': 0.6031550804199074, 'min_child_weight': 6, 'reg_alpha': 1.8531844127913444, 'reg_lambda': 8.999083390810247}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:16:59,183] Trial 16 finished with value: 0.739621919749496 and parameters: {'n_estimators': 531, 'max_depth': 4, 'learning_rate': 0.016725468805498342, 'subsample': 0.69913777059109, 'colsample_bytree': 0.6249118483217919, 'min_child_weight': 8, 'reg_alpha': 2.6493128371576313, 'reg_lambda': 4.378530424829893}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:17:01,908] Trial 17 finished with value: 0.733305548122462 and parameters: {'n_estimators': 346, 'max_depth': 5, 'learning_rate': 0.01737610789734786, 'subsample': 0.9829475907192735, 'colsample_bytree': 0.7035331236949203, 'min_child_weight': 6, 'reg_alpha': 0.1179424662730515, 'reg_lambda': 2.8146189222549203}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:17:04,630] Trial 18 finished with value: 0.7371060891007651 and parameters: {'n_estimators': 491, 'max_depth': 4, 'learning_rate': 0.04238840637425275, 'subsample': 0.8114523201687197, 'colsample_bytree': 0.55308878021856, 'min_child_weight': 3, 'reg_alpha': 6.014599374736709, 'reg_lambda': 0.21734929706430517}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:17:13,630] Trial 19 finished with value: 0.7314298470153265 and parameters: {'n_estimators': 634, 'max_depth': 7, 'learning_rate': 0.016293626895893983, 'subsample': 0.6984990407744667, 'colsample_bytree': 0.5115591906985761, 'min_child_weight': 7, 'reg_alpha': 1.7408871842404907, 'reg_lambda': 6.874105671706218}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:17:25,803] Trial 20 finished with value: 0.7177995393861331 and parameters: {'n_estimators': 314, 'max_depth': 10, 'learning_rate': 0.0369046944028297, 'subsample': 0.8863654731167562, 'colsample_bytree': 0.7247959069005175, 'min_child_weight': 8, 'reg_alpha': 3.3635953296609715, 'reg_lambda': 4.944470147827489}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:17:27,760] Trial 21 finished with value: 0.7433022556207363 and parameters: {'n_estimators': 434, 'max_depth': 3, 'learning_rate': 0.011788845492648766, 'subsample': 0.7625903192458283, 'colsample_bytree': 0.5044440674782779, 'min_child_weight': 10, 'reg_alpha': 5.679221874875048, 'reg_lambda': 3.4833065353702852}. Best is trial 10 with value: 0.7444379945905113.\n",
      "[I 2025-10-27 09:17:29,949] Trial 22 finished with value: 0.744646430240873 and parameters: {'n_estimators': 479, 'max_depth': 3, 'learning_rate': 0.01348335204519964, 'subsample': 0.7163650928019657, 'colsample_bytree': 0.5737999821565282, 'min_child_weight': 9, 'reg_alpha': 6.108678535022071, 'reg_lambda': 3.045558105959864}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:17:32,063] Trial 23 finished with value: 0.7414593945668325 and parameters: {'n_estimators': 485, 'max_depth': 3, 'learning_rate': 0.012833927678432817, 'subsample': 0.8339056762940708, 'colsample_bytree': 0.5760295999127436, 'min_child_weight': 9, 'reg_alpha': 6.053416139610494, 'reg_lambda': 2.9781496700381007}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:17:34,696] Trial 24 finished with value: 0.7423783069662504 and parameters: {'n_estimators': 630, 'max_depth': 3, 'learning_rate': 0.023031713360678923, 'subsample': 0.7142451177501775, 'colsample_bytree': 0.6494925391207395, 'min_child_weight': 10, 'reg_alpha': 5.6497070211986005, 'reg_lambda': 1.5058789006016076}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:17:38,481] Trial 25 finished with value: 0.7366097924853573 and parameters: {'n_estimators': 577, 'max_depth': 5, 'learning_rate': 0.013067299744373827, 'subsample': 0.7879138221351902, 'colsample_bytree': 0.545232600787633, 'min_child_weight': 9, 'reg_alpha': 5.466707304621218, 'reg_lambda': 4.46309074437215}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:17:40,334] Trial 26 finished with value: 0.744312362799134 and parameters: {'n_estimators': 295, 'max_depth': 3, 'learning_rate': 0.019836739185556215, 'subsample': 0.501592385129917, 'colsample_bytree': 0.5929831498671624, 'min_child_weight': 10, 'reg_alpha': 6.663905479767782, 'reg_lambda': 2.90813188720019}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:17:42,318] Trial 27 finished with value: 0.7441940602165714 and parameters: {'n_estimators': 293, 'max_depth': 4, 'learning_rate': 0.02096686925028848, 'subsample': 0.5193413961479807, 'colsample_bytree': 0.6002234673152466, 'min_child_weight': 9, 'reg_alpha': 6.5205796230549, 'reg_lambda': 2.19990383750231}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:17:43,398] Trial 28 finished with value: 0.7040974530570803 and parameters: {'n_estimators': 52, 'max_depth': 3, 'learning_rate': 0.032475954828476684, 'subsample': 0.580305522498071, 'colsample_bytree': 0.6338948666102, 'min_child_weight': 8, 'reg_alpha': 3.7360395106625885, 'reg_lambda': 2.9658510184727973}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:17:56,049] Trial 29 finished with value: 0.725731917180217 and parameters: {'n_estimators': 984, 'max_depth': 7, 'learning_rate': 0.021903782194647857, 'subsample': 0.6585049436994385, 'colsample_bytree': 0.9549496236157864, 'min_child_weight': 9, 'reg_alpha': 7.765742431581695, 'reg_lambda': 1.430258736635055}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:18:07,132] Trial 30 finished with value: 0.7239026830123352 and parameters: {'n_estimators': 809, 'max_depth': 9, 'learning_rate': 0.07473558309522031, 'subsample': 0.587504016057173, 'colsample_bytree': 0.7783676324800263, 'min_child_weight': 10, 'reg_alpha': 4.76364306314339, 'reg_lambda': 0.7975726301361297}. Best is trial 22 with value: 0.744646430240873.\n",
      "[I 2025-10-27 09:18:08,930] Trial 31 finished with value: 0.745485426186247 and parameters: {'n_estimators': 277, 'max_depth': 4, 'learning_rate': 0.021051379105108264, 'subsample': 0.5031867896138733, 'colsample_bytree': 0.598584775039882, 'min_child_weight': 9, 'reg_alpha': 6.741606175567498, 'reg_lambda': 2.340954496680921}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:10,713] Trial 32 finished with value: 0.7377223498681235 and parameters: {'n_estimators': 236, 'max_depth': 4, 'learning_rate': 0.013575752607224744, 'subsample': 0.5255607212127347, 'colsample_bytree': 0.6841289794684837, 'min_child_weight': 9, 'reg_alpha': 6.627140713675562, 'reg_lambda': 2.1886926620295237}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:12,480] Trial 33 finished with value: 0.7416567214189298 and parameters: {'n_estimators': 340, 'max_depth': 3, 'learning_rate': 0.028654922044426122, 'subsample': 0.5039729376819118, 'colsample_bytree': 0.5854667356239434, 'min_child_weight': 8, 'reg_alpha': 7.509157177796439, 'reg_lambda': 1.2459123645623595}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:14,147] Trial 34 finished with value: 0.7153685141050492 and parameters: {'n_estimators': 196, 'max_depth': 4, 'learning_rate': 0.1645416394412356, 'subsample': 0.5493748769287187, 'colsample_bytree': 0.540312965187975, 'min_child_weight': 10, 'reg_alpha': 5.169552449841277, 'reg_lambda': 4.531315819913425}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:16,547] Trial 35 finished with value: 0.7327369845864817 and parameters: {'n_estimators': 265, 'max_depth': 5, 'learning_rate': 0.020434200226661182, 'subsample': 0.6098494761428173, 'colsample_bytree': 0.8620391062079481, 'min_child_weight': 9, 'reg_alpha': 8.401403543257944, 'reg_lambda': 3.2111552230414664}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:21,380] Trial 36 finished with value: 0.7240701186256228 and parameters: {'n_estimators': 480, 'max_depth': 6, 'learning_rate': 0.02537011300591038, 'subsample': 0.8644333472012076, 'colsample_bytree': 0.6749858526970239, 'min_child_weight': 8, 'reg_alpha': 6.310976410799626, 'reg_lambda': 9.66819257317352}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:22,496] Trial 37 finished with value: 0.6838018469700788 and parameters: {'n_estimators': 102, 'max_depth': 3, 'learning_rate': 0.01408333429211608, 'subsample': 0.6680352927618318, 'colsample_bytree': 0.618766532150668, 'min_child_weight': 10, 'reg_alpha': 7.323463606094353, 'reg_lambda': 2.5003420233341194}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:24,747] Trial 38 finished with value: 0.7386186526227783 and parameters: {'n_estimators': 369, 'max_depth': 4, 'learning_rate': 0.01916631006650681, 'subsample': 0.5410277872078655, 'colsample_bytree': 0.5368709636916162, 'min_child_weight': 4, 'reg_alpha': 9.376610276762072, 'reg_lambda': 5.4215171292235045}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:26,013] Trial 39 finished with value: 0.7413406326053901 and parameters: {'n_estimators': 136, 'max_depth': 3, 'learning_rate': 0.038231372792762584, 'subsample': 0.9199226357647152, 'colsample_bytree': 0.5782586220006812, 'min_child_weight': 5, 'reg_alpha': 8.115626916657762, 'reg_lambda': 0.28068418566630804}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:29,430] Trial 40 finished with value: 0.7313548690425251 and parameters: {'n_estimators': 297, 'max_depth': 6, 'learning_rate': 0.02526193540644894, 'subsample': 0.502764901804198, 'colsample_bytree': 0.7221205380694222, 'min_child_weight': 7, 'reg_alpha': 6.8349314230345914, 'reg_lambda': 4.088688830278243}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:31,380] Trial 41 finished with value: 0.7444964777673228 and parameters: {'n_estimators': 287, 'max_depth': 4, 'learning_rate': 0.02023179120165747, 'subsample': 0.5215975969243176, 'colsample_bytree': 0.6048835192944235, 'min_child_weight': 9, 'reg_alpha': 6.3421286113425985, 'reg_lambda': 1.9802943541683646}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:32,949] Trial 42 finished with value: 0.7342723403223614 and parameters: {'n_estimators': 184, 'max_depth': 4, 'learning_rate': 0.013946433491504045, 'subsample': 0.6029558583268568, 'colsample_bytree': 0.5673114913030719, 'min_child_weight': 9, 'reg_alpha': 5.0803575863927986, 'reg_lambda': 1.7906738407558827}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:34,465] Trial 43 finished with value: 0.7448900601654145 and parameters: {'n_estimators': 222, 'max_depth': 3, 'learning_rate': 0.019191793832999328, 'subsample': 0.5674860801997385, 'colsample_bytree': 0.6348909083911972, 'min_child_weight': 9, 'reg_alpha': 4.354992565434581, 'reg_lambda': 0.9793151882007571}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:36,147] Trial 44 finished with value: 0.7429432719161427 and parameters: {'n_estimators': 221, 'max_depth': 4, 'learning_rate': 0.0324452119200262, 'subsample': 0.5600322091952448, 'colsample_bytree': 0.640781783290854, 'min_child_weight': 8, 'reg_alpha': 4.196996646831998, 'reg_lambda': 1.234791608918719}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:39,397] Trial 45 finished with value: 0.744776187114962 and parameters: {'n_estimators': 453, 'max_depth': 5, 'learning_rate': 0.011550244618149162, 'subsample': 0.5307816342394717, 'colsample_bytree': 0.6725912942895715, 'min_child_weight': 9, 'reg_alpha': 4.132796822952363, 'reg_lambda': 0.8167718739483623}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:43,963] Trial 46 finished with value: 0.7403674976678623 and parameters: {'n_estimators': 709, 'max_depth': 5, 'learning_rate': 0.014930516089927395, 'subsample': 0.5746492772529167, 'colsample_bytree': 0.6748931034257415, 'min_child_weight': 9, 'reg_alpha': 3.797335289633429, 'reg_lambda': 0.355399921199705}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:47,880] Trial 47 finished with value: 0.737343120806717 and parameters: {'n_estimators': 573, 'max_depth': 5, 'learning_rate': 0.050942527948262435, 'subsample': 0.5325945238919442, 'colsample_bytree': 0.6173990139467206, 'min_child_weight': 8, 'reg_alpha': 4.596656368401525, 'reg_lambda': 0.8279547241641417}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:52,430] Trial 48 finished with value: 0.7345352921479631 and parameters: {'n_estimators': 446, 'max_depth': 6, 'learning_rate': 0.010014861309039513, 'subsample': 0.6192544231863167, 'colsample_bytree': 0.7002809764672673, 'min_child_weight': 6, 'reg_alpha': 6.027258882873665, 'reg_lambda': 1.8982569479506979}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:54,013] Trial 49 finished with value: 0.6551928651658565 and parameters: {'n_estimators': 109, 'max_depth': 5, 'learning_rate': 0.011475013036064569, 'subsample': 0.6417941158849254, 'colsample_bytree': 0.6639506804076258, 'min_child_weight': 7, 'reg_alpha': 5.476080714768403, 'reg_lambda': 1.0724798954412493}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:55,813] Trial 50 finished with value: 0.7425142855848615 and parameters: {'n_estimators': 258, 'max_depth': 4, 'learning_rate': 0.018359596875530015, 'subsample': 0.5869895836168147, 'colsample_bytree': 0.7450736870600039, 'min_child_weight': 9, 'reg_alpha': 5.027154442443392, 'reg_lambda': 2.419981547270379}. Best is trial 31 with value: 0.745485426186247.\n",
      "[I 2025-10-27 09:18:57,732] Trial 51 finished with value: 0.7459288331154552 and parameters: {'n_estimators': 383, 'max_depth': 3, 'learning_rate': 0.012578348317288563, 'subsample': 0.5524299836441116, 'colsample_bytree': 0.6132578318221937, 'min_child_weight': 10, 'reg_alpha': 4.233162676814973, 'reg_lambda': 0.6098303312532551}. Best is trial 51 with value: 0.7459288331154552.\n",
      "[I 2025-10-27 09:18:59,563] Trial 52 finished with value: 0.7462207084013877 and parameters: {'n_estimators': 369, 'max_depth': 3, 'learning_rate': 0.01517480619148286, 'subsample': 0.5605249781472931, 'colsample_bytree': 0.6104941104203, 'min_child_weight': 9, 'reg_alpha': 3.2697906446497242, 'reg_lambda': 0.5709052515061915}. Best is trial 52 with value: 0.7462207084013877.\n",
      "[I 2025-10-27 09:19:01,346] Trial 53 finished with value: 0.7456631090598926 and parameters: {'n_estimators': 343, 'max_depth': 3, 'learning_rate': 0.014845451120350649, 'subsample': 0.5552445790746635, 'colsample_bytree': 0.6455069795663125, 'min_child_weight': 1, 'reg_alpha': 2.6006046937732368, 'reg_lambda': 0.6698328618626634}. Best is trial 52 with value: 0.7462207084013877.\n",
      "[I 2025-10-27 09:19:03,180] Trial 54 finished with value: 0.7465314105496835 and parameters: {'n_estimators': 333, 'max_depth': 3, 'learning_rate': 0.01567887574810545, 'subsample': 0.5567201950209036, 'colsample_bytree': 0.6473873680578545, 'min_child_weight': 1, 'reg_alpha': 2.520150971964566, 'reg_lambda': 0.04930122854299135}. Best is trial 54 with value: 0.7465314105496835.\n",
      "[I 2025-10-27 09:19:05,013] Trial 55 finished with value: 0.746669111356916 and parameters: {'n_estimators': 347, 'max_depth': 3, 'learning_rate': 0.016306854821207077, 'subsample': 0.5608442880990087, 'colsample_bytree': 0.6479638724993493, 'min_child_weight': 1, 'reg_alpha': 2.596624045095785, 'reg_lambda': 0.10289601963298622}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:06,813] Trial 56 finished with value: 0.7463017613545505 and parameters: {'n_estimators': 352, 'max_depth': 3, 'learning_rate': 0.014928680644787322, 'subsample': 0.5602061807577097, 'colsample_bytree': 0.6123276701365157, 'min_child_weight': 1, 'reg_alpha': 1.1584647406141817, 'reg_lambda': 0.4774668911147343}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:08,563] Trial 57 finished with value: 0.7450171280623702 and parameters: {'n_estimators': 344, 'max_depth': 3, 'learning_rate': 0.01536885688973187, 'subsample': 0.6033717380903982, 'colsample_bytree': 0.6512707902116411, 'min_child_weight': 1, 'reg_alpha': 1.1309032283768463, 'reg_lambda': 0.11758982885687447}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:10,430] Trial 58 finished with value: 0.7456737535142812 and parameters: {'n_estimators': 385, 'max_depth': 3, 'learning_rate': 0.01621591795004604, 'subsample': 0.5509949810909461, 'colsample_bytree': 0.6195470384290317, 'min_child_weight': 1, 'reg_alpha': 2.4080012880848596, 'reg_lambda': 0.4972439258771034}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:12,396] Trial 59 finished with value: 0.6911024216356706 and parameters: {'n_estimators': 394, 'max_depth': 3, 'learning_rate': 0.20590824160934518, 'subsample': 0.627278782435495, 'colsample_bytree': 0.6949051688387345, 'min_child_weight': 2, 'reg_alpha': 1.4855850221782791, 'reg_lambda': 0.37559500434460497}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:14,430] Trial 60 finished with value: 0.7446058076500277 and parameters: {'n_estimators': 382, 'max_depth': 3, 'learning_rate': 0.01716533991684205, 'subsample': 0.5936783790789582, 'colsample_bytree': 0.6233237406540089, 'min_child_weight': 2, 'reg_alpha': 2.359589571028625, 'reg_lambda': 0.03563215068617065}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:16,295] Trial 61 finished with value: 0.7456211182735766 and parameters: {'n_estimators': 336, 'max_depth': 3, 'learning_rate': 0.015463666402193925, 'subsample': 0.5507178407700761, 'colsample_bytree': 0.6527657859440938, 'min_child_weight': 1, 'reg_alpha': 2.8828698450651418, 'reg_lambda': 0.563218982579276}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:18,363] Trial 62 finished with value: 0.7465665124193472 and parameters: {'n_estimators': 415, 'max_depth': 3, 'learning_rate': 0.012411009280090896, 'subsample': 0.5574963876811105, 'colsample_bytree': 0.613808183393689, 'min_child_weight': 2, 'reg_alpha': 3.276894550209167, 'reg_lambda': 0.5444742012658927}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:20,546] Trial 63 finished with value: 0.7449024631656375 and parameters: {'n_estimators': 519, 'max_depth': 3, 'learning_rate': 0.010939128052039417, 'subsample': 0.5681627788656209, 'colsample_bytree': 0.5578709310178456, 'min_child_weight': 2, 'reg_alpha': 3.24052149254691, 'reg_lambda': 1.5461998421770438}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:22,580] Trial 64 finished with value: 0.7462240782344931 and parameters: {'n_estimators': 410, 'max_depth': 3, 'learning_rate': 0.01228769502054234, 'subsample': 0.5490379104340656, 'colsample_bytree': 0.6155743805615518, 'min_child_weight': 1, 'reg_alpha': 0.8995833804833029, 'reg_lambda': 0.533795834428511}. Best is trial 55 with value: 0.746669111356916.\n",
      "[I 2025-10-27 09:19:24,480] Trial 65 finished with value: 0.7473074827006271 and parameters: {'n_estimators': 420, 'max_depth': 3, 'learning_rate': 0.012292026417798353, 'subsample': 0.5772835444787364, 'colsample_bytree': 0.6098067258548637, 'min_child_weight': 2, 'reg_alpha': 0.8541175902190576, 'reg_lambda': 0.14002812109144452}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:26,930] Trial 66 finished with value: 0.7470236742616165 and parameters: {'n_estimators': 423, 'max_depth': 4, 'learning_rate': 0.01233375469005808, 'subsample': 0.5738759743090069, 'colsample_bytree': 0.5891145621514017, 'min_child_weight': 2, 'reg_alpha': 0.7591034891674143, 'reg_lambda': 0.11264313709013217}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:29,296] Trial 67 finished with value: 0.7469377430667518 and parameters: {'n_estimators': 416, 'max_depth': 4, 'learning_rate': 0.01078220142316383, 'subsample': 0.5799078811845421, 'colsample_bytree': 0.5898317361095462, 'min_child_weight': 2, 'reg_alpha': 0.6432264746218401, 'reg_lambda': 0.005132176407082564}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:32,099] Trial 68 finished with value: 0.7464683346650511 and parameters: {'n_estimators': 524, 'max_depth': 4, 'learning_rate': 0.01072555795181536, 'subsample': 0.6759274880306257, 'colsample_bytree': 0.5243670378980312, 'min_child_weight': 3, 'reg_alpha': 0.0761574847323091, 'reg_lambda': 0.011979116322581307}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:34,813] Trial 69 finished with value: 0.7464256209955007 and parameters: {'n_estimators': 520, 'max_depth': 4, 'learning_rate': 0.010826033524898302, 'subsample': 0.6830035588578185, 'colsample_bytree': 0.5278299879311882, 'min_child_weight': 3, 'reg_alpha': 0.10672351765002075, 'reg_lambda': 0.028352636319442154}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:37,497] Trial 70 finished with value: 0.7405323303612317 and parameters: {'n_estimators': 437, 'max_depth': 4, 'learning_rate': 0.01042134497676208, 'subsample': 0.6186520130109887, 'colsample_bytree': 0.5587930024702386, 'min_child_weight': 2, 'reg_alpha': 0.4949824618752603, 'reg_lambda': 6.469061423359683}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:40,296] Trial 71 finished with value: 0.7464660608308007 and parameters: {'n_estimators': 519, 'max_depth': 4, 'learning_rate': 0.011175327514538966, 'subsample': 0.6649148021949026, 'colsample_bytree': 0.5342158183551845, 'min_child_weight': 3, 'reg_alpha': 0.3234568692115949, 'reg_lambda': 0.11905134980738578}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:43,263] Trial 72 finished with value: 0.7456823943354175 and parameters: {'n_estimators': 556, 'max_depth': 4, 'learning_rate': 0.011918366590783134, 'subsample': 0.6390213007409586, 'colsample_bytree': 0.5843926020263593, 'min_child_weight': 3, 'reg_alpha': 0.44950294054417483, 'reg_lambda': 0.09837670021442634}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:46,380] Trial 73 finished with value: 0.7451405288712664 and parameters: {'n_estimators': 602, 'max_depth': 4, 'learning_rate': 0.011790237116581845, 'subsample': 0.667857273807976, 'colsample_bytree': 0.5160159638840566, 'min_child_weight': 2, 'reg_alpha': 0.7158037456455528, 'reg_lambda': 1.2743770094380547}. Best is trial 65 with value: 0.7473074827006271.\n",
      "[I 2025-10-27 09:19:49,130] Trial 74 finished with value: 0.7474460817027635 and parameters: {'n_estimators': 506, 'max_depth': 4, 'learning_rate': 0.010039759088193033, 'subsample': 0.5931751865647532, 'colsample_bytree': 0.5261346790371032, 'min_child_weight': 4, 'reg_alpha': 1.540290017799538, 'reg_lambda': 0.005508102339637805}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:00,680] Trial 75 finished with value: 0.7262601831738357 and parameters: {'n_estimators': 466, 'max_depth': 8, 'learning_rate': 0.013214905668630181, 'subsample': 0.5886004840053806, 'colsample_bytree': 0.5530917310039978, 'min_child_weight': 4, 'reg_alpha': 1.7082414496567442, 'reg_lambda': 7.840080399556829}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:17,997] Trial 76 finished with value: 0.7306893870174674 and parameters: {'n_estimators': 414, 'max_depth': 9, 'learning_rate': 0.0101743409655383, 'subsample': 0.5792790334575958, 'colsample_bytree': 0.5904971626402028, 'min_child_weight': 2, 'reg_alpha': 1.4278431713826, 'reg_lambda': 1.1158463315266391}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:20,630] Trial 77 finished with value: 0.728071558686466 and parameters: {'n_estimators': 493, 'max_depth': 4, 'learning_rate': 0.1105817830870645, 'subsample': 0.6008109294448654, 'colsample_bytree': 0.5235585177723207, 'min_child_weight': 2, 'reg_alpha': 2.1755243422761077, 'reg_lambda': 0.9633400812967214}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:28,399] Trial 78 finished with value: 0.744996118184829 and parameters: {'n_estimators': 494, 'max_depth': 7, 'learning_rate': 0.013115504985712822, 'subsample': 0.617299118481193, 'colsample_bytree': 0.5718381746951643, 'min_child_weight': 4, 'reg_alpha': 0.8676691792678479, 'reg_lambda': 0.008971392478322772}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:30,630] Trial 79 finished with value: 0.7309103601103258 and parameters: {'n_estimators': 312, 'max_depth': 4, 'learning_rate': 0.061728825093700646, 'subsample': 0.5374912432716876, 'colsample_bytree': 0.6342074128932891, 'min_child_weight': 3, 'reg_alpha': 1.3750554025371309, 'reg_lambda': 0.3611719370179286}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:34,380] Trial 80 finished with value: 0.7439862462032641 and parameters: {'n_estimators': 540, 'max_depth': 5, 'learning_rate': 0.017843505629366912, 'subsample': 0.7388386174583117, 'colsample_bytree': 0.5890881704890619, 'min_child_weight': 2, 'reg_alpha': 2.0335969833279464, 'reg_lambda': 1.4193292650031009}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:36,863] Trial 81 finished with value: 0.7465176337620653 and parameters: {'n_estimators': 432, 'max_depth': 4, 'learning_rate': 0.011101561166956682, 'subsample': 0.5762464082666705, 'colsample_bytree': 0.5369258598531352, 'min_child_weight': 3, 'reg_alpha': 0.2580353731804035, 'reg_lambda': 0.338306920798734}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:39,263] Trial 82 finished with value: 0.7464277142755511 and parameters: {'n_estimators': 435, 'max_depth': 4, 'learning_rate': 0.013906460293486082, 'subsample': 0.5192881083912723, 'colsample_bytree': 0.5026826464275616, 'min_child_weight': 3, 'reg_alpha': 0.17444323218861135, 'reg_lambda': 0.8181344022808434}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:41,996] Trial 83 finished with value: 0.7455479722439085 and parameters: {'n_estimators': 467, 'max_depth': 4, 'learning_rate': 0.012492813766342592, 'subsample': 0.5782520797144409, 'colsample_bytree': 0.549853105131789, 'min_child_weight': 4, 'reg_alpha': 0.6682083616656129, 'reg_lambda': 0.29433544361941677}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:44,963] Trial 84 finished with value: 0.7452409660971119 and parameters: {'n_estimators': 415, 'max_depth': 5, 'learning_rate': 0.010119452764497095, 'subsample': 0.6300222752655501, 'colsample_bytree': 0.5670389370260033, 'min_child_weight': 5, 'reg_alpha': 0.002984362592582279, 'reg_lambda': 0.3720167250236553}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:47,530] Trial 85 finished with value: 0.7436396443252509 and parameters: {'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.011142478868415101, 'subsample': 0.5966809202833613, 'colsample_bytree': 0.5373516059579689, 'min_child_weight': 2, 'reg_alpha': 1.110687059708253, 'reg_lambda': 0.8080769804353359}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:50,682] Trial 86 finished with value: 0.7430103976097865 and parameters: {'n_estimators': 502, 'max_depth': 4, 'learning_rate': 0.013781807094921177, 'subsample': 0.5166192534529459, 'colsample_bytree': 0.8176861360370187, 'min_child_weight': 3, 'reg_alpha': 1.608835851281041, 'reg_lambda': 1.0172321438007021}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:52,430] Trial 87 finished with value: 0.7459616186424727 and parameters: {'n_estimators': 318, 'max_depth': 3, 'learning_rate': 0.012122532707042074, 'subsample': 0.6503099552179054, 'colsample_bytree': 0.5200235660694016, 'min_child_weight': 1, 'reg_alpha': 1.9984672129091867, 'reg_lambda': 0.2442400519545202}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:54,946] Trial 88 finished with value: 0.7443878192770683 and parameters: {'n_estimators': 451, 'max_depth': 4, 'learning_rate': 0.01671012524528665, 'subsample': 0.5715780224869289, 'colsample_bytree': 0.5952212775571184, 'min_child_weight': 2, 'reg_alpha': 1.0261771224424272, 'reg_lambda': 1.6041815671243917}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:56,963] Trial 89 finished with value: 0.7456818253936988 and parameters: {'n_estimators': 427, 'max_depth': 3, 'learning_rate': 0.010904671642579637, 'subsample': 0.5384485873609273, 'colsample_bytree': 0.5465753655337487, 'min_child_weight': 3, 'reg_alpha': 0.2764923508153965, 'reg_lambda': 0.6987618860221966}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:20:59,913] Trial 90 finished with value: 0.741427213775039 and parameters: {'n_estimators': 365, 'max_depth': 5, 'learning_rate': 0.01316941315385058, 'subsample': 0.6094129678531517, 'colsample_bytree': 0.7207389075730073, 'min_child_weight': 1, 'reg_alpha': 0.6052432282120849, 'reg_lambda': 1.2773625634063779}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:02,763] Trial 91 finished with value: 0.746027712373195 and parameters: {'n_estimators': 558, 'max_depth': 4, 'learning_rate': 0.011404242374619202, 'subsample': 0.5853152139488946, 'colsample_bytree': 0.5260266440863641, 'min_child_weight': 3, 'reg_alpha': 0.3202050375294646, 'reg_lambda': 0.2131108732345534}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:05,813] Trial 92 finished with value: 0.743729538289711 and parameters: {'n_estimators': 522, 'max_depth': 4, 'learning_rate': 0.010068227069036229, 'subsample': 0.6917084148232999, 'colsample_bytree': 0.8369120654933635, 'min_child_weight': 4, 'reg_alpha': 0.4016506105919832, 'reg_lambda': 0.012782755363937706}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:08,480] Trial 93 finished with value: 0.7452603252687592 and parameters: {'n_estimators': 467, 'max_depth': 4, 'learning_rate': 0.014331981040109255, 'subsample': 0.6581600434523471, 'colsample_bytree': 0.5309660859826427, 'min_child_weight': 2, 'reg_alpha': 1.2832620478904713, 'reg_lambda': 0.284977123540358}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:11,013] Trial 94 finished with value: 0.7445862785355942 and parameters: {'n_estimators': 662, 'max_depth': 3, 'learning_rate': 0.012325793752908066, 'subsample': 0.5696324443599013, 'colsample_bytree': 0.541439621568848, 'min_child_weight': 3, 'reg_alpha': 0.7879925699450955, 'reg_lambda': 0.6912605845950472}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:13,980] Trial 95 finished with value: 0.7426524639676144 and parameters: {'n_estimators': 397, 'max_depth': 5, 'learning_rate': 0.011161661131570058, 'subsample': 0.7200845929701889, 'colsample_bytree': 0.5120089190211999, 'min_child_weight': 2, 'reg_alpha': 2.8437141999176108, 'reg_lambda': 0.47243009475241315}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:16,030] Trial 96 finished with value: 0.7459713640514307 and parameters: {'n_estimators': 324, 'max_depth': 4, 'learning_rate': 0.016074630181467917, 'subsample': 0.6089714038574723, 'colsample_bytree': 0.5001031348051199, 'min_child_weight': 3, 'reg_alpha': 1.881910657285955, 'reg_lambda': 0.21766403346486862}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:18,315] Trial 97 finished with value: 0.7465547963327197 and parameters: {'n_estimators': 509, 'max_depth': 3, 'learning_rate': 0.01073575280347711, 'subsample': 0.5892559708372702, 'colsample_bytree': 0.5780627078736369, 'min_child_weight': 1, 'reg_alpha': 0.9802579751863031, 'reg_lambda': 0.9833966209792867}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:21,546] Trial 98 finished with value: 0.735910998347482 and parameters: {'n_estimators': 839, 'max_depth': 3, 'learning_rate': 0.022470353531792945, 'subsample': 0.5955283447043093, 'colsample_bytree': 0.5783476764033105, 'min_child_weight': 1, 'reg_alpha': 2.2905538004293233, 'reg_lambda': 0.9158906470638779}. Best is trial 74 with value: 0.7474460817027635.\n",
      "[I 2025-10-27 09:21:23,430] Trial 99 finished with value: 0.7457806039539889 and parameters: {'n_estimators': 360, 'max_depth': 3, 'learning_rate': 0.012802603758694962, 'subsample': 0.5425635186357136, 'colsample_bytree': 0.6281852045906121, 'min_child_weight': 1, 'reg_alpha': 2.5917838873913177, 'reg_lambda': 1.09732158211393}. Best is trial 74 with value: 0.7474460817027635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 506, 'max_depth': 4, 'learning_rate': 0.010039759088193033, 'subsample': 0.5931751865647532, 'colsample_bytree': 0.5261346790371032, 'min_child_weight': 4, 'reg_alpha': 1.540290017799538, 'reg_lambda': 0.005508102339637805}\n",
      "Best R2: 0.747\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√°c h√†m make_preprocessor, apply_vt_mask, select_topk, X, y, top_idx, top_k ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n",
    "\n",
    "def select_topk(X_transformed, top_idx=top_idx):\n",
    "    return X_transformed[:, top_idx] if isinstance(X_transformed, np.ndarray) else X_transformed.iloc[:, top_idx]\n",
    "\n",
    "# H√†m objective cho Optuna\n",
    "def objective(trial):\n",
    "    # ƒê·ªãnh nghƒ©a kh√¥ng gian tham s·ªë\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "\n",
    "    # Kh·ªüi t·∫°o danh s√°ch ƒë·ªÉ l∆∞u RMSE v√† R2\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    all_rmse, all_r2 = [], []\n",
    "\n",
    "    # Cross-validation\n",
    "    for tr_idx, te_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_train, y_test = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "        # Preprocessing\n",
    "        preprocessor = make_preprocessor()\n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_t = select_topk(apply_vt_mask(preprocessor.transform(X_train)), top_idx)\n",
    "        X_test_t = select_topk(apply_vt_mask(preprocessor.transform(X_test)), top_idx)\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train_t, y_train)\n",
    "        y_pred = model.predict(X_test_t)\n",
    "\n",
    "        # T√≠nh to√°n metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        all_rmse.append(rmse)\n",
    "        all_r2.append(r2)\n",
    "\n",
    "    # Tr·∫£ v·ªÅ gi√° tr·ªã trung b√¨nh R2 (Optuna t·ªëi ∆∞u h√≥a theo h∆∞·ªõng t·ªëi ƒëa)\n",
    "    return np.mean(all_r2)\n",
    "\n",
    "# T·∫°o study v√† t·ªëi ∆∞u h√≥a\n",
    "study = optuna.create_study(direction='maximize')  # T·ªëi ƒëa h√≥a R2\n",
    "study.optimize(objective, n_trials=100)  # Th·ª≠ 100 t·ªï h·ª£p tham s·ªë\n",
    "\n",
    "# L·∫•y tham s·ªë t·ªët nh·∫•t\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(f\"Best R2: {study.best_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc97fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 506, 'max_depth': 4, 'learning_rate': 0.010039759088193033, 'subsample': 0.5931751865647532, 'colsample_bytree': 0.5261346790371032, 'min_child_weight': 4, 'reg_alpha': 1.540290017799538, 'reg_lambda': 0.005508102339637805}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81ec6990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CV V√íNG 2: Top-100 + Tham s·ªë t·ªëi ∆∞u ===\n",
      "Fold 1: RMSE=2.803  R2=0.698\n",
      "Fold 2: RMSE=2.525  R2=0.720\n",
      "Fold 3: RMSE=2.470  R2=0.778\n",
      "Fold 4: RMSE=2.551  R2=0.750\n",
      "Fold 5: RMSE=2.237  R2=0.790\n",
      "\n",
      "FINAL: RMSE=2.517 ¬± 0.181 | R2=0.747\n"
     ]
    }
   ],
   "source": [
    "# Define the select_topk function\n",
    "def select_topk(X_transformed, top_idx=top_idx):\n",
    "    return X_transformed[:, top_idx] if isinstance(X_transformed, np.ndarray) else X_transformed.iloc[:, top_idx]\n",
    "\n",
    "# ---------- 4) CV v√≤ng 2: REFIT V·ªöI TOP-K + THAM S·ªê T·ªêI ∆ØU ----------\n",
    "xgb_final = XGBRegressor(**best_params, random_state=42, n_jobs=-1, tree_method='hist')\n",
    "all_rmse2, all_r2_2 = [], []\n",
    "\n",
    "print(f\"\\n=== CV V√íNG 2: Top-{top_k} + Tham s·ªë t·ªëi ∆∞u ===\")\n",
    "for fold, (tr_idx, te_idx) in enumerate(tscv.split(X), start=1):\n",
    "    X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    y_train, y_test = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    preprocessor = make_preprocessor()\n",
    "    preprocessor.fit(X_train, y_train)\n",
    "    X_train_t = select_topk(apply_vt_mask(preprocessor.transform(X_train)))\n",
    "    X_test_t  = select_topk(apply_vt_mask(preprocessor.transform(X_test)))\n",
    "\n",
    "    xgb_final.fit(X_train_t, y_train)\n",
    "    y_pred = xgb_final.predict(X_test_t)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    all_rmse2.append(rmse); all_r2_2.append(r2)\n",
    "    print(f\"Fold {fold}: RMSE={rmse:.3f}  R2={r2:.3f}\")\n",
    "\n",
    "print(f\"\\nFINAL: RMSE={np.mean(all_rmse2):.3f} ¬± {np.std(all_rmse2):.3f} | R2={np.mean(all_r2_2):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
