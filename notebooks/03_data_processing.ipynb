{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e945af",
   "metadata": {},
   "source": [
    "# Hanoi Weather Data - Data Processing\n",
    "\n",
    "This notebook handles data cleaning, preprocessing, and preparation for feature engineering and modeling.\n",
    "\n",
    "## Objectives\n",
    "1. Load and clean the raw weather data\n",
    "2. Handle missing values and outliers\n",
    "3. Data type conversions and validation\n",
    "4. Create base temporal features\n",
    "5. Data quality checks and validation\n",
    "6. Export cleaned data for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667736d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba37e59",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_raw = pd.read_csv('../data/raw/daily_data.csv')\n",
    "\n",
    "print(\"📋 RAW DATA OVERVIEW:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Date range: {df_raw['datetime'].min()} to {df_raw['datetime'].max()}\")\n",
    "\n",
    "# Check data types and missing values\n",
    "print(f\"\\n🔍 DATA QUALITY CHECK:\")\n",
    "print(f\"Missing values per column:\")\n",
    "missing_summary = df_raw.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "if len(missing_summary) > 0:\n",
    "    for col, count in missing_summary.items():\n",
    "        pct = (count / len(df_raw)) * 100\n",
    "        print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"  ✅ No missing values found!\")\n",
    "\n",
    "print(f\"\\nDuplicate records: {df_raw.duplicated().sum()}\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\n📊 DATA TYPES:\")\n",
    "for dtype in df_raw.dtypes.value_counts().index:\n",
    "    cols = df_raw.select_dtypes(include=[dtype]).columns.tolist()\n",
    "    print(f\"  {dtype}: {len(cols)} columns\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
